# ğŸ® Reinforcement Learning: Playing Kirby's Dreamland ğŸŒŸ

![Kirby complete level](https://github.com/user-attachments/assets/73091eef-1614-465c-a7f1-7f92a33a9209)



## ğŸ“ Project Overview
Welcome to the Kirby's Dreamland RL Project! This project showcases the implementation of a Reinforcement Learning (RL) agent that learns to play the first level of Kirby's Dreamland. Using a Double Deep Q-Network (DDQN), the agent aims to:
- ğŸŒ  Complete the level by reaching the warp star.
- ğŸ’€ Avoid losing all lives (Game Over).
  
Key features of the project include:
- ğŸ–¥ï¸ Efficient training using a Convolutional Neural Network (CNN) that processes game data as arrays, not raw images.
- ğŸ† A custom reward and penalty system to guide the agentâ€™s progress.
## ğŸ”‘ Key Details
### ğŸŒ Environment
This project uses PyBoy, a Game Boy emulator, as the training environment. Game states are extracted as arrays representing the game field instead of relying on raw image data.
ğŸ® Action Space: Kirby can perform various actions such as:
- Moving left or right
- Jumping
- flying
- absorb in enemies and items
- Attacking enemies
- ğŸ–¼ï¸ Observation Space: The game area is represented as a continuously updated 2D array.
## ğŸ§  Training Details
The RL agent was trained for 25000 epochs, with each episode defined as:
- ğŸ¯ Completion: Kirby reaches the warp star.
- ğŸ’€ Termination: Kirby loses all lives.
- â±ï¸ Timeout: Kirby takes 2500 steps without completing the level.
### ğŸ“Š Rewards and Penalties
The agent is guided by a custom reward system:
- ğŸ¥‡ Rewards:
  - Progressing toward the goal (e.g., moving right).
  - Defeating enemies or completing the level.
- âš ï¸ Penalties:
  - Standing idle or moving away from the goal
  - Losing health or a life.
### ğŸ§© Neural Network Architecture
- ğŸ¤– Model: Double Deep Q-Network (DDQN).
- ğŸŒ€ Feature Extractor: CNN layers to process game field arrays.
- âš¡ Efficiency: By training on arrays instead of raw images, the agent achieves:
  - ğŸš€ Faster training.
  - ğŸ› ï¸ Reduced resource usage.
  - ğŸ“ˆ Maintained accuracy.

 
## âš¡ Why This Approach?
Training directly on raw game images is computationally expensive and often unnecessary. By representing the game state as numerical arrays:
- ğŸ› ï¸ Resources: Significantly reduced computational requirements.
- ğŸš€ Speed: Faster training with efficient memory usage.
- ğŸ¯ Focus: Enables the agent to learn the most relevant patterns in the game environment.

## ğŸš€ Getting Started
### ğŸ”§ Setup
1. Clone the repository:
  `git clone https://github.com/your-repo/kirby-rl.git`
  `cd kirby-rl`
2. Install the dependencies:
   `pip install -r requirements.txt`
3. Place the Kirby.gb ROM file in the appropriate directory.

### ğŸ‹ï¸ Run Training
Start training the agent by running:
`python main.py`
### ğŸ”„ Resume Training
To resume training from the last checkpoint:
- The model will automatically load the latest saved state.
- Exploration (epsilon) will adjust dynamically to continue learning effectively.

## ğŸŒŸ Results
After training for 25000 epochs:
- ğŸ‰ The agent completes the first level by reaching the warp star.
- ğŸ“ˆ Learning is guided by a reward system that encourages progress and discourages inefficient behavior.

## ğŸŒ Future Work
ğŸ’¡ Potential enhancements for the project:
1. ğŸ•¹ï¸ Extend Training: Apply the RL agent to additional levels of the game.
2. ğŸ“Š Compare Algorithms: Experiment with alternative RL models for better performance.
3. ğŸ”§ Reward Tuning: Refine the reward system for more complex scenarios.
